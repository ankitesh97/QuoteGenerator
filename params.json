
{
  "preprocess":{
    "train":0.7,
    "test":0.2,
    "vocab_size":36000,
    "total":78180
  },
  "lstm":{
    "hidden_nodes":10,
    "alpha":1e-3,
    "beta1":0.9,
    "beta2":0.99,
    "offset":1e-8,
    "batch_size":{"val":32,"cmt":"for mini batch gd"},
    "pool_size":{"cmt":"this parameter is for multiprocessing it will divide the batch equally into number of cores available"},
    "epochs":100,
    "process_parallel":"True",
    "training_size":20
  }

}
